<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>ISSA BQAIN</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

							<!-- Logo -->
								<a href="index.html" class="logo">
									<span class="symbol"><img src="images/test1.png" alt="" /></span><span class="title">ISSA BQAIN</span>
								</a>

							<!-- Nav -->
								<nav>
									<ul>
										<li><a href="#menu">Menu</a></li>
									</ul>
								</nav>

						</div>
					</header>

				<!-- Menu -->
					<nav id="menu">
						<h2>Menu</h2>
						<ul>
                            <li><a href="index.html">Home</a></li>
							<li><a href="projects.html">Projects</a></li>
                            <li><a href="generic10.html">Quantized GNN training</a></li>
							<li><a href="generic.html">Electronic Charity box</a></li>
                            <li><a href="generic8.html">SpikeGuard</a></li>
                            <li><a href="generic7.html">Robtic Arm Feeder</a></li>
							<li><a href="generic3.html">Intruder Alarm</a></li>
							<li><a href="generic6.html">Mastermind Algorithm</a></li>
							<li><a href="generic2.html">Desinging An Operational Amplifier</a></li>
							<li><a href="index.html">More Coming Soon...</a></li>
						
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Thesis: Quantized Training For Graph Neural Networks</h1>
                            
                            <P> Graph Neural Networks (GNNs) have gained significant attention in recent years due to their exceptional performance in various domains including social networks, recommender systems, and drug discovery. These models have demonstrated remarkable capabilities in capturing complex relationships and structures within geometric data. Modern applications of GNNs rely on large datasets, which are crucial for capturing the intricate relationships and structures present in complex real-world system, however, training these models requires substantial memory and computational resources which increases with input graph size. 
                            </P>

                            <P>Although there has been some work investigating quantization during inference, there has been little research into the quantized training of GNNs. In this project, the quantized training of GNNs is explored using fixed-point quantization and Microsoft Floating Point (MSFP). To achieve optimal results, a range of strategies are employed including dynamic quantization. The results demonstrate that it is possible to achieve model accuracies which are within 1% of baseline FP32 trained models using 8-bit fixed-point quantization, offering an arithmetic and memory density increase of up to 7.7x and 4x respectively. These results can be replicated using Microsoft Floating Point quantization, offering an arithmetic and memory density increase of up to 18.3x and 5.8x respectively. By employing dynamic quantization schemes utilizing Microsoft Floating Point, accuracies which are within 1% of baseline 32-bit floating point  are obtained offering potential gains in arithmetic and memory density of up to 26x and 6.4x respectively.
                            </P>							
							
						</div>
				
					</div>


				<!-- Footer -->
					

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>